{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kor nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset kor_nli/multi_nli to /home/seungbinyang/.cache/huggingface/datasets/kor_nli/multi_nli/1.0.0/06d9b61bd1372a618df02294965857ff10886d48696f33a32cbea656b71dfcf0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 41.7M/41.7M [00:04<00:00, 9.01MB/s]\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset kor_nli downloaded and prepared to /home/seungbinyang/.cache/huggingface/datasets/kor_nli/multi_nli/1.0.0/06d9b61bd1372a618df02294965857ff10886d48696f33a32cbea656b71dfcf0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 116.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"kor_nli\", 'multi_nli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': '개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.',\n",
       " 'hypothesis': '제품과 지리학은 크림 스키밍을 작동시키는 것이다.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need label 0 (entailment)\n",
    "kor_nli_dataset = []\n",
    "\n",
    "for data in dataset['train']:\n",
    "    if data['label'] == 0:\n",
    "        kor_nli_dataset.append({\n",
    "            'question':data['premise'],\n",
    "            'passage':data['hypothesis']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130899"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kor_nli_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '하우스보트는 영국 라지의 전성기의 아름답게 보존된 전통이다.',\n",
       " 'passage': '하우스보트의 전통은 영국 라지가 여전히 강해지는 동안 시작되었다.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_nli_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '그래, 하지만 나는 그것이 내가 매일 세계가 어떤 상태에 있는지 검토하고 우리가 어디로 가고 있는지 추측하는 데 도움이 된다고 생각해.',\n",
       " 'passage': '그것은 내가 미래를 예측하고 매일 무슨 일이 일어나고 있는지 이해하는 데 도움이 된다.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data split\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "random.shuffle(kor_nli_dataset)\n",
    "kor_nli_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "kor_nli_train_dataset = {'data':kor_nli_dataset[:int(len(kor_nli_dataset) * 0.8)]}\n",
    "kor_nli_valid_dataset = {'data':kor_nli_dataset[int(len(kor_nli_dataset) * 0.8):int(len(kor_nli_dataset) * 0.9)]}\n",
    "kor_nli_test_dataset = {'data':kor_nli_dataset[int(len(kor_nli_dataset) * 0.9):]}\n",
    "\n",
    "dir_path = 'kor_nli'\n",
    "\n",
    "with open(f'{dir_path}/train.json', 'w') as outfile:\n",
    "    json.dump(kor_nli_train_dataset, outfile, indent=4, ensure_ascii=False)\n",
    "with open(f'{dir_path}/valid.json', 'w') as outfile:\n",
    "    json.dump(kor_nli_valid_dataset, outfile, indent=4, ensure_ascii=False)\n",
    "with open(f'{dir_path}/test.json', 'w') as outfile:\n",
    "    json.dump(kor_nli_test_dataset, outfile, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kor_quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 6.12k/6.12k [00:00<00:00, 15.2MB/s]\n",
      "Found cached dataset squad_kor_v1 (/home/seungbinyang/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_kor_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 60407\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5774\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "valid_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for data in dataset['train']:\n",
    "    train_dataset.append({\n",
    "        'question':data['question'],\n",
    "        'passage':data['context']\n",
    "    })\n",
    "\n",
    "for data in dataset['validation']:\n",
    "    valid_dataset.append({\n",
    "        'question':data['question'],\n",
    "        'passage':data['context']\n",
    "    })\n",
    "\n",
    "test_dataset = train_dataset[int(len(train_dataset)*0.9):]\n",
    "train_dataset = train_dataset[:int(len(train_dataset)*0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54366, 5774, 6041)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "kor_squad_train_dataset = {'data':train_dataset}\n",
    "kor_squad_valid_dataset = {'data':valid_dataset}\n",
    "kor_squad_test_dataset = {'data':test_dataset}\n",
    "\n",
    "dir_path = 'kor_squad'\n",
    "\n",
    "with open(f'{dir_path}/train.json', 'w') as outfile:\n",
    "    json.dump(kor_squad_train_dataset, outfile, indent=4, ensure_ascii=False)\n",
    "with open(f'{dir_path}/valid.json', 'w') as outfile:\n",
    "    json.dump(kor_squad_valid_dataset, outfile, indent=4, ensure_ascii=False)\n",
    "with open(f'{dir_path}/test.json', 'w') as outfile:\n",
    "    json.dump(kor_squad_test_dataset, outfile, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
